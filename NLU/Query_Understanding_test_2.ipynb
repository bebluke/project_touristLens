{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Davlan/xlm-roberta-base-ner-hrl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOCATION': ['é˜¿é‡Œå±±'], 'ADDRESS': [], 'POI_TYPE': [], 'NEED': [], 'search_type': ['geo_filter']}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "def load_nlu_models():\n",
    "    \"\"\"è¼‰å…¥ NERï¼ˆXLM-RoBERTaï¼‰èˆ‡ Query åˆ†é¡ï¼ˆmT5ï¼‰æ¨¡å‹\"\"\"\n",
    "    # NER æ¨¡å‹ï¼ˆXLM-RoBERTaï¼Œä½¿ç”¨æ–°æ¨¡å‹ Davlan/xlm-roberta-base-ner-hrlï¼‰\n",
    "    ner_model_name = \"Davlan/xlm-roberta-base-ner-hrl\"\n",
    "    ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
    "    ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_name)\n",
    "    ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "    \n",
    "    # Query é¡åˆ¥åˆ†é¡ï¼ˆmT5ï¼‰\n",
    "    query_model_name = \"google/mt5-small\"\n",
    "    query_tokenizer = T5Tokenizer.from_pretrained(query_model_name, model_max_length=512)\n",
    "    query_model = MT5ForConditionalGeneration.from_pretrained(query_model_name, device_map=\"auto\")\n",
    "    \n",
    "    return ner_pipeline, query_model, query_tokenizer\n",
    "\n",
    "def extract_entities(query, ner_pipeline):\n",
    "    \"\"\"ä½¿ç”¨ XLM-RoBERTa NER æ¨¡å‹ä¾†æå– LOCATIONã€POI_TYPEã€NEEDã€ADDRESSï¼Œä¸¦åˆä½µå­è©\"\"\"\n",
    "    entities = ner_pipeline(query)\n",
    "    \n",
    "    # åˆå§‹åŒ–çµæœ\n",
    "    result = {\"LOCATION\": [], \"ADDRESS\": [], \"POI_TYPE\": [], \"NEED\": []}\n",
    "    label_map = {\"LOC\": \"LOCATION\", \"ORG\": \"POI_TYPE\", \"MISC\": \"NEED\"}\n",
    "    \n",
    "    prev_label, prev_word = None, \"\"\n",
    "    for entity in entities:\n",
    "        word = entity['word'].replace(\"â–\", \"\").strip()  # å»æ‰ç‰¹æ®Šç¬¦è™Ÿä¸¦å»é™¤ç©ºæ ¼\n",
    "        label = entity['entity']  # BIO æ ¼å¼æ¨™ç±¤\n",
    "        main_label = label[2:] if \"-\" in label else label  # ç§»é™¤ B- æˆ– I-ï¼Œä¿ç•™é¡åˆ¥\n",
    "        mapped_label = label_map.get(main_label, None)  # è½‰æ›æˆæˆ‘å€‘çš„åˆ†é¡\n",
    "        \n",
    "        if not mapped_label or not word:\n",
    "            continue  # å¦‚æœæ¨™ç±¤ä¸åœ¨æˆ‘å€‘çš„åˆ†é¡ä¸­æˆ–æ˜¯ç©ºå­—ä¸²ï¼Œè·³é\n",
    "        \n",
    "        # BIO è½‰æ›ï¼Œå°‡ B- å’Œ I- åˆä½µè™•ç†\n",
    "        if label.startswith(\"B-\"):\n",
    "            if prev_label and prev_word.strip():\n",
    "                result[prev_label].append(prev_word)  # å­˜å…¥å‰ä¸€å€‹è©\n",
    "            prev_word = word\n",
    "            prev_label = mapped_label  # å­˜å„²å°æ‡‰çš„æ¨™ç±¤\n",
    "        elif label.startswith(\"I-\") and prev_label == mapped_label:\n",
    "            prev_word += word  # åˆä½µè©\n",
    "        else:\n",
    "            if prev_label and prev_word.strip():\n",
    "                result[prev_label].append(prev_word)  # å­˜å…¥å‰ä¸€å€‹è©\n",
    "            prev_label, prev_word = None, \"\"\n",
    "    \n",
    "    if prev_label and prev_word.strip():\n",
    "        result[prev_label].append(prev_word)  # å­˜å…¥æœ€å¾Œä¸€å€‹è©\n",
    "    \n",
    "    # å¦‚æœ LOCATION æ˜¯ç¸£å¸‚ï¼Œå°±ç§»åˆ° ADDRESS\n",
    "    location_as_address = {\"å°åŒ—å¸‚\", \"æ–°åŒ—å¸‚\", \"å°ä¸­å¸‚\", \"å°å—å¸‚\", \"é«˜é›„å¸‚\", \"æ¡ƒåœ’å¸‚\", \"å®œè˜­ç¸£\", \"æ–°ç«¹å¸‚\"}\n",
    "    for loc in result[\"LOCATION\"][:]:\n",
    "        if loc in location_as_address:\n",
    "            result[\"LOCATION\"].remove(loc)\n",
    "            result[\"ADDRESS\"].append(loc)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def classify_query_type(query, query_model, query_tokenizer):\n",
    "    \"\"\"ä½¿ç”¨ mT5 æ¨¡å‹ä¾†åˆ†é¡ Query é¡å‹ï¼ˆgeo_filter / geo_distanceï¼‰\"\"\"\n",
    "    input_ids = query_tokenizer(query, return_tensors=\"pt\").input_ids.to(query_model.device)\n",
    "    outputs = query_model.generate(input_ids, max_length=10, num_return_sequences=1, do_sample=False)\n",
    "    result = query_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if result not in [\"geo_filter\", \"geo_distance\"]:\n",
    "        result = \"geo_filter\"  # é è¨­ç‚º geo_filter\n",
    "    \n",
    "    return {\"search_type\": [result]}\n",
    "\n",
    "def nlu_pipeline(query):\n",
    "    \"\"\"å®Œæ•´ NLU æµç¨‹ï¼šNER + Query é¡å‹åˆ†é¡\"\"\"\n",
    "    ner_pipeline, query_model, query_tokenizer = load_nlu_models()\n",
    "    \n",
    "    entities = extract_entities(query, ner_pipeline)\n",
    "    query_type = classify_query_type(query, query_model, query_tokenizer)\n",
    "    \n",
    "    return {**entities, **query_type}\n",
    "\n",
    "# æ¸¬è©¦\n",
    "query = \"é˜¿é‡Œå±±æœ‰å“ªäº›å’–å•¡é¤¨ï¼Ÿ\"\n",
    "result = nlu_pipeline(query)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ•ˆæœå°šå¯ï¼Œé‚„æœ‰é€²æ­¥ç©ºé–“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOCATION': ['SunMoonLake'], 'ADDRESS': [], 'POI_TYPE': ['é¤å»³'], 'NEED': [], 'search_type': ['unknown']}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def load_nlu_models():\n",
    "    \"\"\"è¼‰å…¥ NERï¼ˆXLM-RoBERTaï¼‰èˆ‡ Query åˆ†é¡ï¼ˆmT5ï¼‰æ¨¡å‹\"\"\"\n",
    "    # NER æ¨¡å‹ï¼ˆXLM-RoBERTaï¼Œä½¿ç”¨æ–°æ¨¡å‹ Davlan/xlm-roberta-base-ner-hrlï¼‰\n",
    "    ner_model_name = \"Davlan/xlm-roberta-base-ner-hrl\"\n",
    "    ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
    "    ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_name)\n",
    "    ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "    \n",
    "    # Query é¡åˆ¥åˆ†é¡ï¼ˆmT5ï¼‰\n",
    "    query_model_name = \"google/mt5-small\"\n",
    "    query_tokenizer = T5Tokenizer.from_pretrained(query_model_name, model_max_length=512)\n",
    "    query_model = MT5ForConditionalGeneration.from_pretrained(query_model_name, device_map=\"auto\")\n",
    "    \n",
    "    return ner_pipeline, query_model, query_tokenizer\n",
    "\n",
    "# åˆå§‹åŒ–èªæ„å‘é‡æ¨¡å‹\n",
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# å®šç¾© POI é¡å‹é—œéµå­—\n",
    "POI_KEYWORDS = [\n",
    "    \"é¤å»³\", \"ä¸­é¤é¤¨\", \"äºæ´²èœé¤å»³\", \"ç«é‹é¤å»³\", \"å¤œå¸‚\", \"ç”œå“åº—\", \"å†°å“é£²æ–™åº—\", \"ç¾é£Ÿå»£å ´\",\n",
    "    \"å’–å•¡é¤¨\", \"èŒ¶è‘‰åº—\", \"èŒ¶å¸‚\", \"èŒ¶æ‰¹ç™¼å•†\", \"èŒ¶è£½é€ å•†\", \"é…’å“å°ˆè³£åº—\", \"é‡€é…’å» \", \"é…’æ¨“\", \"é£¯ç›’ä¾›æ‡‰å•†\",\n",
    "    \"é…’åº—\", \"è³“é¤¨\", \"æ—…é¤¨\", \"æ°‘å®¿\", \"æ¸¡å‡æ‘\", \"é•·æœŸä½å®¿é…’åº—\", \"å®—æ•™ä½å®¿å ´æ‰€\",\n",
    "    \"æ—…éŠæ™¯é»\", \"è§€å…‰ç‰§å ´\", \"è§€å…‰è¾²å ´\", \"è‡ªç„¶ä¿è­·å€\", \"åœ‹å®¶å…¬åœ’\", \"åœ‹å®¶æ£®æ—\", \"æ°´ä¸Šæ¨‚åœ’\", \"æµ·æ¿±é•·å»Š\", \"æ²™ç˜\",\n",
    "    \"æ¹–æ³Š\", \"æ²³æµ\", \"å³¶å¶¼\", \"å±±å³°\", \"éºå€åšç‰©é¤¨\", \"æ­·å²æ™¯é»\", \"æ–‡åŒ–åœ°æ¨™\", \"ç´€å¿µå…¬åœ’\", \"ç´€å¿µç¢‘\",\n",
    "    \"ç™»å±±çºœè»Š\", \"è¡Œå±±å¾‘\", \"æº«æ³‰\", \"éœ²ç‡Ÿåœ°é»\", \"é‡£é­šæ± \",\"å…¬åœ’\",\n",
    "    \"åšç‰©é¤¨\", \"æ­·å²åšç‰©é¤¨\", \"ç§‘å­¸é¤¨\", \"è—è¡“åšç‰©é¤¨\", \"æ‰‹å·¥è—åšç‰©é¤¨\", \"å‹•ç‰©å­¸åšç‰©é¤¨\", \"æµ·äº‹åšç‰©é¤¨\",\n",
    "    \"éŸ³æ¨‚å»³\", \"æ­ŒåŠ‡é™¢\", \"æ¼”è—åŠ‡å ´\", \"å±•è¦½å ´åœ°\", \"å±•è¦½è²¿æ˜“ä¸­å¿ƒ\", \"è—è¡“ä¸­å¿ƒ\", \"æ–‡åŒ–ä¸­å¿ƒ\",\n",
    "    \"ç™¾è²¨å…¬å¸\", \"è³¼ç‰©ä¸­å¿ƒ\", \"å¸‚å ´\", \"è¾²ç”¢å“å¸‚å ´\", \"æµ·é®®å¸‚å ´\", \"ç´€å¿µå“å•†åº—\", \"ç¦®å“åº—\", \"æ›¸åº—\", \"å¤è‘£åº—\",\n",
    "    \"å¯ºå»Ÿ\", \"å¤©ä¸»æ•™æ•™å ‚\", \"ç¥ç¤¾\", \"é“è§€\", \"å®—æ•™è–åœ°\", \"ç¥å£‡\",\n",
    "    \"ç«è»Šç«™\", \"æ¸¡è¼ªç¢¼é ­\", \"æ©Ÿç¥¨ä»£ç†å…¬å¸\", \"æ©‹æ¨‘\", \"éš§é“\", \"éµè·¯å…¬å¸\", \"éµé“æœå‹™\", \"åœè»Šå ´\",\n",
    "    \"éŠæ¨‚å ´\", \"ä¸»é¡Œå…¬åœ’\", \"æ‘©å¤©è¼ª\", \"å‹•ç‰©åœ’\", \"é‡ç”Ÿå‹•ç‰©åœ’\", \"é«”è‚²é¤¨\", \"æ¼†å½ˆå°„æ“Šå ´\", \"æ½›æ°´ä¸­å¿ƒ\", \"å–®è»Šå¾‘\"\n",
    "]\n",
    "\n",
    "# é è¨ˆç®— POI é—œéµå­—å‘é‡\n",
    "poi_embeddings = embedding_model.encode(POI_KEYWORDS, convert_to_numpy=True)\n",
    "faiss_index = faiss.IndexFlatL2(poi_embeddings.shape[1])\n",
    "faiss_index.add(poi_embeddings)\n",
    "\n",
    "def detect_poi_type(query):\n",
    "    \"\"\"ä½¿ç”¨èªæ„å‘é‡æª¢ç´¢ POI_TYPEï¼Œé¸æ“‡æœ€ç¬¦åˆçš„é¡åˆ¥\"\"\"\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    D, I = faiss_index.search(query_embedding, 3)  # å–æœ€ç›¸è¿‘çš„ 3 å€‹ POI é¡å‹\n",
    "    \n",
    "    # éæ¿¾ï¼šå„ªå…ˆé¸æ“‡è¼ƒå»£æ³›çš„ POI é¡å‹\n",
    "    preferred_types = {\"é¤å»³\", \"ç¾é£Ÿå»£å ´\", \"å¤œå¸‚\", \"æ—…éŠæ™¯é»\", \"å¸‚å ´\"}\n",
    "    for idx in I[0]:\n",
    "        if idx >= 0 and POI_KEYWORDS[idx] in preferred_types:\n",
    "            return [POI_KEYWORDS[idx]]\n",
    "    \n",
    "    # è‹¥æ‰¾ä¸åˆ°é©åˆçš„å»£æ³›é¡å‹ï¼Œå‰‡å›å‚³æœ€ç›¸ä¼¼çš„çµæœ\n",
    "    return [POI_KEYWORDS[I[0][0]]] if I[0][0] >= 0 else []\n",
    "\n",
    "def nlu_pipeline(query):\n",
    "    \"\"\"å®Œæ•´ NLU æµç¨‹ï¼šNER + Query é¡å‹åˆ†é¡ + POI_TYPE èªæ„æª¢ç´¢\"\"\"\n",
    "    ner_pipeline, query_model, query_tokenizer = load_nlu_models()\n",
    "    \n",
    "    entities = extract_entities(query, ner_pipeline)\n",
    "    query_type = classify_query_type(query, query_model, query_tokenizer)\n",
    "    \n",
    "    # **ä½¿ç”¨èªæ„æª¢ç´¢ POI_TYPE**\n",
    "    if not entities[\"POI_TYPE\"]:  \n",
    "        entities[\"POI_TYPE\"] = detect_poi_type(query)\n",
    "    \n",
    "    return {**entities, **query_type}\n",
    "\n",
    "# æ¸¬è©¦\n",
    "query = \"I'm looking for a takeaway restaurant or snack near Sun Moon Lake\"\n",
    "result = nlu_pipeline(query)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  å˜—è©¦è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='565' max='565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [565/565 01:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>11.342694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.695917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.318912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.222490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10.493400</td>\n",
       "      <td>0.885306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled mT5 è¨“ç·´å®Œæˆ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOCATION': ['SunMoonLake'], 'ADDRESS': [], 'POI_TYPE': ['é¤å»³'], 'NEED': [], 'search_type': ['geo_filter']}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "def load_nlu_models():\n",
    "    \"\"\"è¼‰å…¥ NERï¼ˆXLM-RoBERTaï¼‰èˆ‡ Query åˆ†é¡ï¼ˆDistil-mT5ï¼‰æ¨¡å‹\"\"\"\n",
    "    # NER æ¨¡å‹ï¼ˆXLM-RoBERTaï¼‰\n",
    "    ner_model_name = \"Davlan/xlm-roberta-base-ner-hrl\"\n",
    "    ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
    "    ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_name)\n",
    "    ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "    \n",
    "    # Query é¡åˆ¥åˆ†é¡ï¼ˆDistil-mT5ï¼‰\n",
    "    query_model_name = \"google/mt5-small\"\n",
    "    query_tokenizer = T5Tokenizer.from_pretrained(query_model_name, model_max_length=128)\n",
    "    query_model = MT5ForConditionalGeneration.from_pretrained(query_model_name, device_map=\"auto\")\n",
    "    \n",
    "    return ner_pipeline, query_model, query_tokenizer\n",
    "\n",
    "# åˆå§‹åŒ–èªæ„å‘é‡æ¨¡å‹\n",
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "def load_training_data(file_path, tokenizer):\n",
    "    \"\"\"å¾ CSV æ–‡ä»¶è¼‰å…¥è¨“ç·´æ•¸æ“šï¼Œä¸¦å° Query é€²è¡Œ Tokenization\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # è½‰æ›æ–‡å­—æ¨™ç±¤ï¼ˆgeo_filter / geo_distanceï¼‰ç‚º Token æ ¼å¼\n",
    "    label_texts = df[\"label\"].tolist()\n",
    "    tokenized_labels = tokenizer(label_texts, padding=True, truncation=True, max_length=10, return_tensors=\"pt\")\n",
    "    \n",
    "    # å° Query é€²è¡Œ Tokenization\n",
    "    tokenized_data = tokenizer(df[\"query\"].tolist(), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    \n",
    "    # å»ºç«‹ Dataset æ ¼å¼\n",
    "    dataset = datasets.Dataset.from_dict({\n",
    "        \"input_ids\": tokenized_data[\"input_ids\"].tolist(),\n",
    "        \"attention_mask\": tokenized_data[\"attention_mask\"].tolist(),\n",
    "        \"labels\": tokenized_labels[\"input_ids\"].tolist()\n",
    "    })\n",
    "    \n",
    "    return dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "training_file_path = \"geo_training_data.csv\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "data_splits = load_training_data(training_file_path, tokenizer)\n",
    "train_data = data_splits[\"train\"]\n",
    "eval_data = data_splits[\"test\"]\n",
    "\n",
    "def train_distilled_mt5():\n",
    "    \"\"\"è¨“ç·´ Distil-mT5 é€²è¡Œ `geo_filter` vs `geo_distance` åˆ†é¡\"\"\"\n",
    "    train_args = TrainingArguments(\n",
    "        output_dir=\"./distilled_mt5\", per_device_train_batch_size=8, num_train_epochs=5,\n",
    "        logging_dir=\"./logs\", save_steps=500, evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5, weight_decay=0.01, save_total_limit=2\n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\"))\n",
    "    trainer = Trainer(\n",
    "        model=MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\"),\n",
    "        args=train_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(\"./distilled_mt5\")\n",
    "    print(\"Distilled mT5 è¨“ç·´å®Œæˆ\")\n",
    "\n",
    "train_distilled_mt5()\n",
    "\n",
    "# æ¸¬è©¦\n",
    "query = \"I'm looking for a takeaway restaurant or snack near Sun Moon Lake\"\n",
    "result = nlu_pipeline(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "tokenizer.save_pretrained(\"./distilled_mt5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'search_type': ['unknown']}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "def classify_query_type(query, query_model, query_tokenizer):\n",
    "    \"\"\"ä½¿ç”¨ `distilled_mt5` é€²è¡ŒæŸ¥è©¢é¡åˆ¥åˆ†é¡\"\"\"\n",
    "    input_ids = query_tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids.to(query_model.device)\n",
    "    \n",
    "    \n",
    "    outputs = query_model.generate(\n",
    "        input_ids, \n",
    "        max_length=20, \n",
    "        num_return_sequences=1,\n",
    "        do_sample=False,   \n",
    "        num_beams=5        \n",
    "    )\n",
    "    \n",
    "    result = query_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    \n",
    "    \n",
    "    if result.startswith(\"<extra_id_\") or result == \"\":\n",
    "        result = \"unknown\"\n",
    "    \n",
    "    return {\"search_type\": [result]}\n",
    "\n",
    "# æ¸¬è©¦ `distilled_mt5`\n",
    "query = \"I'm looking for a takeaway restaurant or snack near Sun Moon Lake\"\n",
    "result = classify_query_type(query, query_model, query_tokenizer)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'es_query': '<extra_id_0>_distance'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "def load_trained_mt5():\n",
    "    \"\"\"è¼‰å…¥å·²è¨“ç·´çš„ Distilled mT5\"\"\"\n",
    "    query_model_name = \"./distilled_mt5\"  \n",
    "    query_tokenizer = T5Tokenizer.from_pretrained(query_model_name)\n",
    "    query_model = MT5ForConditionalGeneration.from_pretrained(query_model_name, device_map=\"auto\")\n",
    "    return query_model, query_tokenizer\n",
    "\n",
    "def generate_es_query(query_model, query_tokenizer, location, poi_type, need, distance):\n",
    "    \"\"\"ç”¨ mT5 ç”¢ç”Ÿ Elasticsearch Query DSL\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    è«‹æ ¹æ“šä»¥ä¸‹æ¢ä»¶ç”Ÿæˆ Elasticsearch Queryï¼š\n",
    "    åœ°é»ï¼šã€Œ{location}ã€\n",
    "    é¡åˆ¥ï¼šã€Œ{poi_type}ã€\n",
    "    éœ€æ±‚ï¼šã€Œ{need}ã€\n",
    "    æœå°‹ç¯„åœï¼šã€Œ{distance} å…§ã€\n",
    "    è«‹è¼¸å‡ºç¬¦åˆ Elasticsearch DSL æ ¼å¼çš„ JSON æŸ¥è©¢èªå¥ï¼š\n",
    "    \"\"\"\n",
    "    \n",
    "    input_ids = query_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids.to(query_model.device)\n",
    "    outputs = query_model.generate(input_ids, max_length=256)\n",
    "    result = query_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return {\"es_query\": result}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #  è¼‰å…¥æ¨¡å‹\n",
    "    query_model, query_tokenizer = load_trained_mt5()\n",
    "    \n",
    "    #  æ¸¬è©¦ Elasticsearch Query ç”Ÿæˆ\n",
    "    test_query = generate_es_query(query_model, query_tokenizer, \"å°åŒ—101\", \"é¤å»³\", \"æ™¯è§€\", \"5km\")\n",
    "    print(test_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'es_query': '<extra_id_0>:'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "def generate_es_query(query_model, query_tokenizer, location, poi_type, need, distance):\n",
    "    \"\"\"ç”¨ mT5 ç”¢ç”Ÿ Elasticsearch Query DSL\"\"\"\n",
    "    \n",
    "    # è®“ mT5 å­¸æœƒ JSON çµæ§‹\n",
    "    example_query = \"\"\"\n",
    "    è«‹æ ¹æ“šä»¥ä¸‹æ¢ä»¶ç”Ÿæˆ Elasticsearch Queryï¼š\n",
    "    åœ°é»ï¼šã€Œé˜¿é‡Œå±±ã€\n",
    "    é¡åˆ¥ï¼šã€Œå’–å•¡é¤¨ã€\n",
    "    éœ€æ±‚ï¼šã€Œæ™¯è§€ã€\n",
    "    æœå°‹ç¯„åœï¼šã€Œ5km å…§ã€\n",
    "    è¼¸å‡ºï¼š\n",
    "    {\n",
    "      \"query\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            { \"match\": { \"gmap_location\": \"é˜¿é‡Œå±±\" } },\n",
    "            { \"match\": { \"class\": \"å’–å•¡é¤¨\" } }\n",
    "          ],\n",
    "          \"filter\": [\n",
    "            { \"geo_distance\": { \"distance\": \"5km\", \"gmap_coordinates\": { \"lat\": 23.508, \"lon\": 120.802 } } }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    {example_query}\n",
    "    ç¾åœ¨è«‹æ ¹æ“šä»¥ä¸‹æ¢ä»¶ç”Ÿæˆ Elasticsearch Queryï¼š\n",
    "    åœ°é»ï¼šã€Œ{location}ã€\n",
    "    é¡åˆ¥ï¼šã€Œ{poi_type}ã€\n",
    "    éœ€æ±‚ï¼šã€Œ{need}ã€\n",
    "    æœå°‹ç¯„åœï¼šã€Œ{distance} å…§ã€\n",
    "    è¼¸å‡ºï¼š\n",
    "    \"\"\"\n",
    "    \n",
    "    input_ids = query_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids.to(query_model.device)\n",
    "    outputs = query_model.generate(input_ids, max_length=256)\n",
    "    result = query_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return {\"es_query\": result}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #  è¼‰å…¥æ¨¡å‹ï¼ˆæ”¹ç”¨ `google/mt5-small`ï¼‰\n",
    "    query_model, query_tokenizer = load_mt5()\n",
    "    \n",
    "    #  æ¸¬è©¦ Elasticsearch Query ç”Ÿæˆ\n",
    "    test_query = generate_es_query(query_model, query_tokenizer, \"å°åŒ—101\", \"é¤å»³\", \"æ™¯è§€\", \"5km\")\n",
    "    print(test_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å˜—è©¦Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ç”¢ç”Ÿçš„ Elasticsearch Query:\n",
      " ```json\n",
      "{\n",
      "  \"query\": {\n",
      "    \"bool\": {\n",
      "      \"must\": [\n",
      "        {\n",
      "          \"match\": {\n",
      "            \"class\": \"å’–å•¡å»³\"\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"filter\": [\n",
      "        {\n",
      "          \"geo_distance\": {\n",
      "            \"distance\": \"5km\",  // å¯èª¿æ•´è·é›¢\n",
      "            \"gmap_coordinates\": {\n",
      "              \"lat\": 23.500754,  // é˜¿é‡Œå±±çš„ç·¯åº¦ï¼Œéœ€è¦å¯¦éš›æŸ¥è©¢\n",
      "              \"lon\": 120.802632   // é˜¿é‡Œå±±çš„ç¶“åº¦ï¼Œéœ€è¦å¯¦éš›æŸ¥è©¢\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "**èªªæ˜:**\n",
      "\n",
      "1. **`query` - `bool` - `must`**:  ä½¿ç”¨ `match` æŸ¥è©¢ `class` æ¬„ä½ï¼Œç¢ºä¿çµæœåŒ…å« \"å’–å•¡å»³\"ã€‚\n",
      "2. **`query` - `bool` - `filter`**: ä½¿ç”¨ `geo_distance` ç¯©é¸å™¨ï¼Œæ ¹æ“šæä¾›çš„é˜¿é‡Œå±±ç¶“ç·¯åº¦åº§æ¨™ï¼ŒæŸ¥æ‰¾é™„è¿‘çš„çµæœã€‚`distance` è¨­å®šç‚º \"5km\"ï¼Œå¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´æœç´¢åŠå¾‘ã€‚**æ³¨æ„ï¼šé˜¿é‡Œå±±çš„ç¶“ç·¯åº¦éœ€è¦æ›¿æ›æˆå¯¦éš›çš„ç¶“ç·¯åº¦å€¼ã€‚**  ä½¿ç”¨ `filter` context ç¢ºä¿æ•ˆèƒ½ï¼Œå› ç‚º `filter` ä¸è¨ˆç®—åˆ†æ•¸ã€‚\n",
      "3. **`filter` ç‚º list æ ¼å¼**: ç¬¦åˆé¡Œç›®è¦æ±‚ï¼Œå³ä½¿åªæœ‰ä¸€å€‹ filter ä¹Ÿä½¿ç”¨ list åŒ…è£¹ã€‚\n",
      "\n",
      "\n",
      "**ä½¿ç”¨æ–¹æ³•:**\n",
      "\n",
      "1. å°‡é˜¿é‡Œå±±çš„ç¶“ç·¯åº¦æ›¿æ›æˆæ­£ç¢ºçš„å€¼ã€‚\n",
      "2. å°‡ JSON æŸ¥è©¢èªå¥è¤‡è£½åˆ° Elasticsearch çš„æœå°‹ API ä¸­åŸ·è¡Œã€‚\n",
      "3. èª¿æ•´ `distance` åƒæ•¸ä»¥æ§åˆ¶æœç´¢åŠå¾‘ã€‚\n",
      "\n",
      "\n",
      "**é¡å¤–èªªæ˜:**\n",
      "\n",
      "* å¦‚æœéœ€è¦æ›´ç²¾ç¢ºçš„ã€Œé™„è¿‘ã€å®šç¾©ï¼Œå¯ä»¥è€ƒæ…®ä½¿ç”¨æ›´å°çš„ `distance` å€¼ã€‚\n",
      "* å¯ä»¥æ ¹æ“šéœ€æ±‚æ·»åŠ å…¶ä»– `must` æˆ– `filter` æ¢ä»¶ï¼Œä¾‹å¦‚æ ¹æ“š `tags` ç¯©é¸ã€Œé©åˆå…’ç«¥ã€çš„å’–å•¡å»³ã€‚\n",
      "*  å¦‚æœæ²’æœ‰é˜¿é‡Œå±±çš„ç²¾ç¢ºç¶“ç·¯åº¦ï¼Œå¯ä»¥ä½¿ç”¨ `gmap_location` æ¬„ä½æ­é… `match` æŸ¥è©¢ï¼Œå…ˆæ‰¾åˆ°é˜¿é‡Œå±±çš„åœ°é»ï¼Œå†æå–å…¶ç¶“ç·¯åº¦åº§æ¨™é€²è¡Œ `geo_distance` æŸ¥è©¢ã€‚  ä½†é€™ç¨®æ–¹æ³•çš„æ•ˆèƒ½ä¸å¦‚ç›´æ¥ä½¿ç”¨ç¶“ç·¯åº¦ã€‚\n",
      "\n",
      "\n",
      "é€™å€‹è§£ç­”æä¾›äº†ä¸€å€‹åŸºæœ¬çš„æ¡†æ¶ï¼Œæ‚¨å¯ä»¥æ ¹æ“šå¯¦éš›éœ€æ±‚é€²è¡Œèª¿æ•´å’Œæ“´å±•ã€‚\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "\n",
    "# è¨­å®šä½ çš„ Gemini API Key\n",
    "genai.configure(api_key=\"\")\n",
    "\n",
    "def generate_es_query_gemini(query):\n",
    "    \"\"\"ä½¿ç”¨ Gemini ç”¢ç”Ÿç¬¦åˆ Elasticsearch Mapping çš„ Queryï¼Œç¢ºä¿ `filter` æ˜¯é™£åˆ—\"\"\"\n",
    "    schema_info = \"\"\"\n",
    "    ä½ çš„ Elasticsearch è³‡æ–™çµæ§‹å¦‚ä¸‹ï¼š\n",
    "    - `gmap_location`: åœ°é»åç¨± (å¦‚ \"å°åŒ—101\")\n",
    "    - `location_ID`: Google åœ°é» ID\n",
    "    - `class`: åœ°é»é¡å‹ (å¦‚ \"é¤å»³\", \"å’–å•¡é¤¨\", \"æ—…éŠæ™¯é»\")\n",
    "    - `address`: åœ°å€\n",
    "    - `summary_1`: ç°¡ä»‹\n",
    "    - `tags`: æ¨™ç±¤ (å¦‚ \"é©åˆå…’ç«¥\", \"æ™¯è§€å„ªç¾\", \"ç„¡éšœç¤™è¨­æ–½\")\n",
    "    - `gmap_coordinates`: ç¶“ç·¯åº¦åº§æ¨™ (lat, lon)\n",
    "\n",
    "    **è«‹ç¢ºä¿ç”Ÿæˆçš„ Elasticsearch Query åªåŒ…å«é€™äº›æ¬„ä½ï¼Œä¸” `filter` å¿…é ˆæ˜¯ `list` æ ¼å¼ã€‚**\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    {schema_info}\n",
    "    è«‹å°‡ä»¥ä¸‹è‡ªç„¶èªè¨€æŸ¥è©¢è½‰æ›ç‚ºç¬¦åˆ Elasticsearch JSON Query DSLï¼š\n",
    "    æŸ¥è©¢ï¼šã€Œ{query}ã€\n",
    "    ç”Ÿæˆç¬¦åˆ JSON æ ¼å¼çš„ Elasticsearch æŸ¥è©¢èªå¥ï¼Œ**è«‹ç¢ºä¿ `filter` æ˜¯ `list` æ ¼å¼**ï¼š\n",
    "    \"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro\")  # ä½¿ç”¨ Gemini 1.5 Pro\n",
    "    response = model.generate_content(prompt)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "# æ¸¬è©¦\n",
    "query = \"æˆ‘æƒ³æ‰¾é˜¿é‡Œå±±é™„è¿‘çš„å’–å•¡å»³ï¼Ÿ\"\n",
    "es_query = generate_es_query_gemini(query)\n",
    "print(\"ğŸ” ç”¢ç”Ÿçš„ Elasticsearch Query:\\n\", es_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# è¨­å®šGemini API Key\n",
    "genai.configure(api_key=\"\")\n",
    "\n",
    "# è¨­å®šå…¨åŸŸè®Šæ•¸ï¼Œçµ±ä¸€ä½¿ç”¨ Gemini æ¨¡å‹\n",
    "GEMINI_MODEL = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "# é€£æ¥ Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "INDEX_NAME = \"gmap_location\"  \n",
    "\n",
    "def find_location_in_database(query):\n",
    "    \"\"\"æŸ¥è©¢åœ°é»æ˜¯å¦å­˜åœ¨æ–¼ Elasticsearchï¼Œè¿”å›ç¶“ç·¯åº¦æˆ– None\"\"\"\n",
    "    search_body = {\n",
    "        \"query\": {\n",
    "            \"match_phrase\": {\"gmap_location\": query}\n",
    "        }\n",
    "    }\n",
    "    response = es.search(index=INDEX_NAME, body=search_body)\n",
    "    \n",
    "    if response[\"hits\"][\"total\"][\"value\"] > 0:\n",
    "        location_data = response[\"hits\"][\"hits\"][0][\"_source\"]\n",
    "        return {\n",
    "            \"gmap_location\": location_data[\"gmap_location\"],\n",
    "            \"gmap_coordinates\": location_data[\"gmap_coordinates\"]\n",
    "        }\n",
    "    return \"None\"\n",
    "\n",
    "def generate_es_query_gemini(query):\n",
    "    \"\"\"ç¶œåˆæµç¨‹ï¼Œç”¢ç”Ÿ Elasticsearch Queryï¼Œä¸¦åŸ·è¡ŒæŸ¥è©¢\"\"\"\n",
    "    location_data = find_location_in_database(query)\n",
    "    \n",
    "    if location_data == \"None\":\n",
    "        raise ValueError(\"ç„¡æ³•æ‰¾åˆ°åœ°é»ï¼Œè«‹æª¢æŸ¥æŸ¥è©¢å…§å®¹ï¼\")\n",
    "    \n",
    "    center_point = location_data.get(\"gmap_coordinates\", None)\n",
    "    if center_point is None:\n",
    "        raise ValueError(\"å›å‚³çš„åœ°é»ç¼ºå°‘ `gmap_coordinates`ï¼Œæª¢æŸ¥ API å›æ‡‰\")\n",
    "    \n",
    "    search_type = \"geo_distance\" \n",
    "    \n",
    "    schema_info = \"\"\"\n",
    "    Elasticsearch Index çµæ§‹å¦‚ä¸‹ï¼š\n",
    "    - `gmap_location`: åœ°é»åç¨± (å¦‚ \"å°åŒ—101\")\n",
    "    - `class`: åœ°é»é¡å‹ (å¦‚ \"é¤å»³\", \"å’–å•¡é¤¨\", \"æ—…éŠæ™¯é»\")\n",
    "    - `gmap_coordinates`: ç¶“ç·¯åº¦åº§æ¨™ (lat, lon)\n",
    "    **è«‹ç¢ºä¿ç”Ÿæˆçš„ Elasticsearch Query åªåŒ…å«é€™äº›æ¬„ä½ï¼Œä¸¦ç¬¦åˆ JSON æ ¼å¼ã€‚**\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {schema_info}\n",
    "    è«‹æ ¹æ“šä»¥ä¸‹æ¢ä»¶ç”Ÿæˆ Elasticsearch Queryï¼š\n",
    "    - åœ°é»ï¼šã€Œ{query}ã€\n",
    "    - æŸ¥è©¢é¡å‹ï¼šã€Œ{search_type}ã€\n",
    "    - åƒè€ƒç¶“ç·¯åº¦ï¼šã€Œ{center_point}ã€\n",
    "    - æœå°‹ç¯„åœï¼šã€Œ1kmã€\n",
    "    **è«‹è¼¸å‡ºå®Œæ•´çš„ JSON æŸ¥è©¢èªå¥ï¼Œä¸è¦é¡å¤–è§£é‡‹ã€‚**\n",
    "    \"\"\"\n",
    "    response = GEMINI_MODEL.generate_content(prompt)\n",
    "    es_query = response.text.strip()\n",
    "    \n",
    "    # åŸ·è¡Œ Elasticsearch æŸ¥è©¢\n",
    "    try:\n",
    "        es_response = es.search(index=INDEX_NAME, body=json.loads(es_query))\n",
    "        return es_response[\"hits\"][\"hits\"]\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"JSON è§£æéŒ¯èª¤ï¼ŒGemini å›å‚³å…§å®¹ç„¡æ³•è§£æ:\\n{es_query}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"é˜¿é‡Œå±±æœ‰å“ªäº›å’–å•¡é¤¨ï¼Ÿ\"\n",
    "    try:\n",
    "        results = generate_es_query_gemini(query)\n",
    "        print(\"Elasticsearch æŸ¥è©¢çµæœ:\")\n",
    "        for result in results:\n",
    "            print(result[\"_source\"])\n",
    "    except ValueError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: I would like find the nearest museum of Shilin Night Market\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"gmap_location\": \"å£«æ—å¤œå¸‚\",\n",
      "  \"class\": \"åšç‰©é¤¨\",\n",
      "  \"geo_distance\": \"é™„è¿‘\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Gemini API Key\n",
    "genai.configure(api_key=\"\")\n",
    "\n",
    "# è¨­å®šå…¨åŸŸè®Šæ•¸ï¼Œä½¿ç”¨ Gemini æ¨¡å‹\n",
    "GEMINI_MODEL = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "\n",
    "def parse_query_with_gemini(query, chat):\n",
    "    \"\"\"è®“ Gemini è§£æ Query ä¸¦è¼¸å‡ºçµæ§‹åŒ– JSON\"\"\"\n",
    "    prompt = f\"\"\"    \n",
    "    åšç‚ºNLPè§£æå™¨ï¼Œä»¥åŠå°ç£çš„æ—…éŠå°ˆå®¶ï¼Œè§£æä»¥ä¸‹ä½¿ç”¨è€…æŸ¥è©¢ï¼Œä¸¦è¼¸å‡ºæ¨™ç±¤èˆ‡å…§å®¹ï¼š\n",
    "    \n",
    "    **æ¨™ç±¤**ï¼š\n",
    "    - `gmap_location`: ç¢ºåˆ‡åœ°é»åç¨±\n",
    "    - `address`: åœ°å€\n",
    "    - `class`: google mapä¸Šçš„åœ°é»é¡å‹\n",
    "    - `opening_hours`: ç‡Ÿæ¥­æ™‚é–“\n",
    "    - `entrance_fee`: é–€ç¥¨ã€å…è²»ç­‰\n",
    "    - `tags`: google mapçš„åœ°é»æ¨™ç±¤ï¼Œå¦‚\"é©åˆå…’ç«¥\"ã€\"ç„¡éšœç¤™åœè»Šå ´\"ã€\"æ´—æ‰‹é–“\"ã€Wi-Fi\"\n",
    "    - `geo_distance`: åœ°ç†è·é›¢æœå°‹çš„åŠå¾‘ï¼Œå¦‚\"1å…¬é‡Œ\"ã€\"é™„è¿‘\"\n",
    "    - `semantic_keywords`: èªæ„é—œéµè©(ç”¨æ–¼èªæ„æª¢ç´¢)ã€å…¶ä»–ä¸å±¬æ–¼ä¸Šè¿°è³‡æ–™çµæ§‹çš„è©ï¼Œæˆ–æ˜¯ä»»ä½•å¯èƒ½åœ¨google mapè©•è«–å‡ºç¾çš„è©ï¼Œå¦‚\"å®‰éœçš„\"ã€\"æµ·é‚Šçœ‹å¤•é™½\"\n",
    "      \n",
    "     \n",
    "    \n",
    "    ä»¥ä¸‹æ˜¯ä¸€äº›è§£æç¯„ä¾‹ï¼š\n",
    "\n",
    "    queryï¼šã€Œå—æ¸¯é©åˆå°å­©å­çš„åšç‰©é¤¨ï¼Œæœ‰ç„¡éšœç¤™åœè»Šå ´ã€ \n",
    "    è¼¸å‡ºï¼š\n",
    "        \"address\": \"å—æ¸¯å€\",        \n",
    "        \"class\": \"åšç‰©é¤¨\",\n",
    "        \"tags\": [\"ç„¡éšœç¤™åœè»Šå ´\",\"é©åˆå…’ç«¥\"]\n",
    "    \n",
    "    queryï¼šã€Œæ¨è–¦åœ‹çˆ¶ç´€å¿µé¤¨5å…¬é‡Œå…§çš„å’–å•¡åº—ï¼Œé€±äºŒä¸‹åˆæœ‰é–‹çš„ã€\n",
    "    è¼¸å‡ºï¼š\n",
    "        \"gmap_location\": \"åœ‹çˆ¶ç´€å¿µé¤¨\",        \n",
    "        \"class\": \"å’–å•¡é¤¨\",\n",
    "        \"geo_distance\": \"5km\",\n",
    "        \"opening_hours\":[\"é€±äºŒ\",\"ä¸‹åˆ\"]\n",
    "    \n",
    "    queryï¼šã€Œå¤§å®‰å’Œä¿¡ç¾©å€æœ‰å“ªäº›å…è²»çš„ç¾è¡“é¤¨ï¼Œè¦æœ‰ç¾ä»£è¨­è¨ˆçš„å±•è¦½å…§å®¹ã€\n",
    "    è¼¸å‡ºï¼š\n",
    "        \"address\": \"å¤§å®‰å€\",\"ä¿¡ç¾©å€\"        \n",
    "        \"class\": \"è—è¡“åšç‰©é¤¨\",\"ç¾ä»£è—è¡“åšç‰©é¤¨\"\n",
    "        \"entrance_fee\": \"free\",\n",
    "        \"semantic_keywords\":[\"ç¾ä»£\",\"è¨­è¨ˆ\"]\n",
    "\n",
    "    queryï¼šã€Œæ¨è–¦ä¸€é–“å°æ±é æµ·é‚Šå®‰éœçš„é¤å»³ã€\n",
    "    è¼¸å‡ºï¼š\n",
    "        \"address\": \"å°æ±ç¸£\",\"å°æ±å¸‚\"        \n",
    "        \"class\": \"é¤å»³\",        \n",
    "        \"semantic_keywords\":[\"å®‰éœ\",\"é æµ·\",\"æµ·é‚Š\"]\n",
    "    \n",
    "    **å°ç£æ‰€æœ‰çš„ç¸£å¸‚éƒ½è¦è§£æç‚ºaddressï¼Œå¦‚å°ä¸­=å°ä¸­å¸‚ã€æ–°ç«¹=\"æ–°ç«¹ç¸£\"ã€\"æ–°ç«¹å¸‚\"**\n",
    "    **\"gmap_location\"å’Œ\"address\"ä¸åŒæ™‚å‡ºç¾åœ¨åŒä¸€è¼¸å‡ºï¼Œæ²’æœ‰æ˜ç¢ºåœ°é»æ‰é¸æ“‡\"address\"\n",
    "    **classè¦å°æ‡‰google mapä¸Šçš„åœ°é»é¡å‹**\n",
    "    **å¦‚æœæœ‰å°è·é›¢çš„æè¿°ï¼Œå¦‚\"é™„è¿‘\"ã€\"å‘¨é‚Š\"è«‹ä½¿ç”¨\"geo_distance\": \"5km\"**\n",
    "    *è¼¸å…¥å¤–æ–‡è¦è½‰æ›æˆä¸­æ–‡ï¼Œé™¤äº†\"semantic_keywords\"ä¿ç•™åŸæ–‡*\n",
    "\n",
    "    è«‹å°‡ä»¥ä¸‹æŸ¥è©¢è§£æ\n",
    "    ã€Œ{query}ã€\n",
    "\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # response = GEMINI_MODEL.generate_content(prompt)\n",
    "    response = chat.send_message(prompt)\n",
    "    \n",
    "   # åˆ¤æ–·æ˜¯å¦æˆåŠŸå›å‚³\n",
    "    if response.parts:\n",
    "        try:\n",
    "            parsed_query = json.loads(response.text.strip())\n",
    "            return parsed_query\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(f\"\\n{response.text.strip()}\")\n",
    "    else:\n",
    "        raise ValueError(f\"å›å‚³å¤±æ•—\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # å»ºç«‹æ–°çš„å°è©±\n",
    "    chat = GEMINI_MODEL.start_chat()\n",
    "\n",
    "    # æ¸¬è©¦ä¸åŒçš„æŸ¥è©¢èªå¥\n",
    "    queries = [        \n",
    "        \"I would like find the nearest museum of Shilin Night Market\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        try:\n",
    "            result = parse_query_with_gemini(query, chat)\n",
    "            print(\"è§£æçµæœ:\")\n",
    "            print(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "        time.sleep(1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query: å°åŒ—å¸‚æœ‰å“ªäº›å…è²»çš„åšç‰©é¤¨ï¼Ÿ\n",
    "{\"address\": \"å°åŒ—å¸‚\",\"class\": \"åšç‰©é¤¨\",\"entrance_fee\": \"å…è²»\"}\n",
    "\n",
    "Query: å“ªè£¡æœ‰é©åˆè¦ªå­çš„å…¬åœ’ï¼Ÿ\n",
    "{\"class\": \"å…¬åœ’\",\"tags\": \"é©åˆå…’ç«¥\"}\n",
    "\n",
    "Query: æˆ‘æƒ³æ‰¾é™½æ˜å±±çš„å’–å•¡é¤¨\n",
    "{\"location\": \"é™½æ˜å±±\",\"type\":\"gmap_location\",\"class\": \"å’–å•¡é¤¨\"}\n",
    "\n",
    "Query: æˆ‘æƒ³æ‰¾ä¿¡ç¾©å€ 1 å…¬é‡Œå…§çš„å¤œå¸‚\n",
    "{\"location\": \"ä¿¡ç¾©å€\",\"type\": \"address\",\"class\": \"å¤œå¸‚\",\"geo_distance\": 1km\"}\n",
    "\n",
    "Query: æ¨è–¦ä¸€å€‹å®‰éœçš„é¤å»³\n",
    "{\"class\": \"é¤å»³\",\"semantic_keywords\":['å®‰éœ']}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ—¥æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sudachipy sudachidict_full sudachidict_core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²æˆåŠŸéæ¿¾ä¸¦è¼¸å‡ºç‚º cleaned_comments_th.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = 'cleaned_comments_new.csv'  \n",
    "output_file = 'cleaned_comments_th.csv'  \n",
    "\n",
    "# è¼‰å…¥ CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# éæ¿¾ \"language\"\n",
    "df_filtered = df[df['language'] == 'th']\n",
    "\n",
    "# è¼¸å‡º\n",
    "df_filtered.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"å·²æˆåŠŸéæ¿¾ä¸¦è¼¸å‡ºç‚º {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import stopwordsiso\n",
    "import gc\n",
    "\n",
    "\n",
    "from sudachipy import Dictionary\n",
    "tokenizer_ja = Dictionary().create() \n",
    "\n",
    "# æ¨¡å‹è¼‰å…¥\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True, use_cuda=True, device=0)\n",
    "\n",
    "# ä¸­æ–‡åœç”¨è©\n",
    "stopwords = stopwordsiso.stopwords(\"ja\")\n",
    "\n",
    "# SudachiPy å­è©åˆä½µå‡½å¼ï¼ˆæ‰¹æ¬¡è™•ç†ç‰ˆæœ¬ï¼‰\n",
    "def merge_subwords_ja(tokens, weights, sudachi_words):\n",
    "    \"\"\"\n",
    "    é‡å°æ—¥æ–‡è©•è«–ï¼š\n",
    "    - XLM-R tokenizer è² è²¬ subword åˆ‡å‰²\n",
    "    - SudachiPy æä¾›è©å½™é‚Šç•Œï¼Œå¹«åŠ©åˆä½µå­è©\n",
    "    \"\"\"\n",
    "    merged_tokens, merged_weights = [], []\n",
    "    idx = 0\n",
    "\n",
    "    for word in sudachi_words:  # ğŸ”¹ é€™è£¡ç›´æ¥ç”¨å·²ç¶“åˆ†è©å¥½çš„ `sudachi_words`\n",
    "        current_weight = 0.0\n",
    "        current_length = 0\n",
    "\n",
    "        while current_length < len(word) and idx < len(tokens):\n",
    "            token = tokens[idx].replace(\"â–\", \"\").replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n",
    "            current_weight += weights[idx]\n",
    "            current_length += len(token)\n",
    "            idx += 1\n",
    "\n",
    "        merged_tokens.append(word)\n",
    "        merged_weights.append(current_weight)\n",
    "\n",
    "    return merged_tokens, merged_weights\n",
    "\n",
    "# è®€å–åœ°é»å°æ‡‰è³‡è¨Š\n",
    "with open(\"../RAW_DATA/E_838_location_info.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    location_info = json.load(f)\n",
    "location_mapping = {loc[\"location_id\"]: loc[\"gmap_location\"] for loc in location_info}\n",
    "\n",
    "location_topics_weight = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "# åˆ†æ‰¹è™•ç†è©•è«–è³‡æ–™ (batch è™•ç† BGE å’Œ CKIP)\n",
    "chunksize = 10000\n",
    "reader = pd.read_csv(\"cleaned_comments_run.csv\", chunksize=chunksize)\n",
    "\n",
    "chunk_index = 0\n",
    "\n",
    "for df_chunk in reader:\n",
    "    df_chunk = df_chunk.dropna(subset=[\"comments\"])\n",
    "    print(f\"æ­£åœ¨è™•ç†ç¬¬ {chunk_index+1} å€‹chunkï¼Œå…± {len(df_chunk)} ç­†è©•è«–\")\n",
    "\n",
    "    comments = df_chunk[\"comments\"].tolist()\n",
    "    location_ids = df_chunk[\"location_id\"].tolist()\n",
    "    languages = df_chunk[\"language\"].tolist()\n",
    "\n",
    "    # ** å…ˆç”¨ tokenizer è™•ç† input_ids (é¿å… KeyError)**\n",
    "    tokenized_inputs = tokenizer(comments, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # ** BGE æ¨¡å‹ batch è™•ç†**\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encode(comments, return_sparse=True, batch_size=128)\n",
    "\n",
    "    sparse_weights_list = outputs[\"lexical_weights\"]  \n",
    "    input_ids_list = tokenized_inputs[\"input_ids\"].tolist()  \n",
    "\n",
    "    # ** SudachiPy åˆ†è© batch è™•ç†**\n",
    "    sudachi_words_batch = [ [m.surface() for m in tokenizer_ja.tokenize(text)] for text in comments ]\n",
    "\n",
    "    # ** è™•ç†æ¯ç­†è©•è«–**\n",
    "    for idx in range(len(comments)):\n",
    "        sparse_weights = sparse_weights_list[idx]\n",
    "        input_ids = input_ids_list[idx]\n",
    "        tokens_str = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        lexical_weights = [sparse_weights.get(str(token_id), 0.0) for token_id in input_ids]\n",
    "\n",
    "        # ä½¿ç”¨ SudachiPy ä¾†æŒ‡å°æ—¥æ–‡å­è©åˆä½µï¼ˆé€™è£¡ä¸å† tokenizeï¼Œå› ç‚º sudachi_words_batch[idx] å·²ç¶“æ˜¯ listï¼‰\n",
    "        merged_tokens, merged_weights = merge_subwords_ja(tokens_str, lexical_weights, sudachi_words_batch[idx])\n",
    "\n",
    "        key = (location_ids[idx], languages[idx])\n",
    "        for token, weight in zip(merged_tokens, merged_weights):\n",
    "            token = token.lower().strip()\n",
    "            if len(token) < 2 or token in stopwords:\n",
    "                continue\n",
    "            location_topics_weight[key][token] += weight\n",
    "\n",
    "    # æ¯è™•ç†å®Œä¸€å€‹chunkï¼Œæ¸…ç†è¨˜æ†¶é«”\n",
    "    del df_chunk, comments, location_ids, languages, sparse_weights_list, input_ids_list, sudachi_words_batch\n",
    "    gc.collect()\n",
    "    chunk_index += 1\n",
    "    print(f\"å®Œæˆç¬¬{chunk_index}å€‹chunkè™•ç†èˆ‡è¨˜æ†¶é«”æ¸…ç†ã€‚\")\n",
    "\n",
    "# å…¨éƒ¨chunkå®Œæˆå¾Œå†ç”¢å‡ºtop30 Excel\n",
    "rows = []\n",
    "for (loc_id, lang), token_weights in location_topics_weight.items():\n",
    "    top30 = sorted(token_weights.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "    top_words_str = \", \".join([f\"{token}:{weight:.2f}\" for token, weight in top30])\n",
    "    rows.append({\n",
    "        \"location_id\": loc_id,\n",
    "        \"gmap_location\": location_mapping.get(loc_id, loc_id),\n",
    "        \"language\": lang,\n",
    "        \"top_30_tokens\": top_words_str\n",
    "    })\n",
    "\n",
    "df_output = pd.DataFrame(rows)\n",
    "df_output.to_excel(\"location_topics_top30_ja.xlsx\", index=False)\n",
    "print(\"è™•ç†å®Œæˆï¼Œå·²å„²å­˜ location_topics_top30_ja.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ³°æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import stopwordsiso\n",
    "import gc\n",
    "\n",
    "# æ¨¡å‹è¼‰å…¥\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True, use_cuda=True, device=0)\n",
    "\n",
    "\n",
    "# ä¸­æ–‡åœç”¨è©\n",
    "stopwords = stopwordsiso.stopwords(\"th\")\n",
    "\n",
    "# pythainlp å­è©åˆä½µå‡½å¼ï¼ˆæ‰¹æ¬¡è™•ç†ç‰ˆæœ¬ï¼‰\n",
    "def merge_subwords_th(tokens, weights, text):\n",
    "    \"\"\"\n",
    "    é‡å°æ³°æ–‡è©•è«–ï¼š\n",
    "    - XLM-R tokenizer å…ˆåˆ‡ subword\n",
    "    - PyThaiNLP å¹«åŠ©åˆä½µå­è©ï¼Œä½¿è©å½™æ›´è‡ªç„¶\n",
    "    \"\"\"\n",
    "    if isinstance(text, list):  # ç¢ºä¿ text æ˜¯ string è€Œé list\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    words = word_tokenize(text, engine=\"newmm\")  \n",
    "    words = [w.strip() for w in words if w.strip()]  # **éæ¿¾æ‰ç©ºç™½è©**\n",
    "\n",
    "    merged_tokens, merged_weights = [], []\n",
    "    idx = 0\n",
    "\n",
    "    for word in words:\n",
    "        current_weight = 0.0\n",
    "        current_length = 0\n",
    "        success = False  # **åŠ å…¥é€™å€‹è®Šæ•¸ä¾†ç¢ºèªæ˜¯å¦æˆåŠŸåŒ¹é…**\n",
    "\n",
    "        while current_length < len(word) and idx < len(tokens):\n",
    "            token = tokens[idx].replace(\"â–\", \"\").replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\").strip()\n",
    "            \n",
    "            if not token:  \n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "            current_weight += weights[idx]\n",
    "            current_length += len(token)\n",
    "            idx += 1\n",
    "            success = True  # **å¦‚æœæœ‰ä»»ä½• token è¢«åŠ ç¸½ï¼Œä»£è¡¨åŒ¹é…æˆåŠŸ**\n",
    "\n",
    "        # **å¦‚æœè©å½™æˆåŠŸåŒ¹é…ï¼Œæ‰åŠ å…¥**\n",
    "        if success and word:\n",
    "            merged_tokens.append(word)\n",
    "            merged_weights.append(current_weight)\n",
    "        elif word:  # **åªåœ¨å®Œå…¨æ²’æœ‰åŒ¹é…æˆåŠŸæ™‚ï¼Œæ‰å°å‡ºè­¦å‘Š**\n",
    "            print(f\"merge_subwords_th() è­¦å‘Šï¼šæœªæˆåŠŸåˆä½µè©å½™ - {word}\")\n",
    "\n",
    "    return merged_tokens, merged_weights\n",
    "\n",
    "# è®€å–åœ°é»å°æ‡‰è³‡è¨Š\n",
    "with open(\"../RAW_DATA/E_838_location_info.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    location_info = json.load(f)\n",
    "location_mapping = {loc[\"location_id\"]: loc[\"gmap_location\"] for loc in location_info}\n",
    "\n",
    "location_topics_weight = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "# åˆ†æ‰¹è™•ç†è©•è«–è³‡æ–™ (batch è™•ç† BGE å’Œ CKIP)\n",
    "chunksize = 10000\n",
    "reader = pd.read_csv(\"cleaned_comments_run.csv\", chunksize=chunksize)\n",
    "\n",
    "chunk_index = 0\n",
    "\n",
    "for df_chunk in reader:\n",
    "    df_chunk = df_chunk.dropna(subset=[\"comments\"])\n",
    "    print(f\"æ­£åœ¨è™•ç†ç¬¬ {chunk_index+1} å€‹chunkï¼Œå…± {len(df_chunk)} ç­†è©•è«–\")\n",
    "\n",
    "    comments = df_chunk[\"comments\"].tolist()\n",
    "    location_ids = df_chunk[\"location_id\"].tolist()\n",
    "    languages = df_chunk[\"language\"].tolist()\n",
    "\n",
    "    # ** å…ˆç”¨ tokenizer è™•ç† input_ids (é¿å… KeyError)**\n",
    "    tokenized_inputs = tokenizer(comments, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # ** BGE æ¨¡å‹ batch è™•ç†**\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encode(comments, return_sparse=True, batch_size=128)\n",
    "\n",
    "    sparse_weights_list = outputs[\"lexical_weights\"]  \n",
    "    input_ids_list = tokenized_inputs[\"input_ids\"].tolist()  \n",
    "\n",
    "    # ** ä¾èªè¨€é¸æ“‡åˆé©çš„åˆ†è©æ–¹æ³•**\n",
    "    thai_words_batch = []\n",
    "\n",
    "    for text in comments:\n",
    "        thai_words_batch.append(word_tokenize(text, engine=\"newmm\"))  # æ³°æ–‡ç”¨ PyThaiNLP åˆ†è©\n",
    "\n",
    "    # ** è™•ç†æ¯ç­†è©•è«–**\n",
    "    for idx in range(len(comments)):\n",
    "        sparse_weights = sparse_weights_list[idx]\n",
    "        input_ids = input_ids_list[idx]\n",
    "        tokens_str = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        lexical_weights = [sparse_weights.get(str(token_id), 0.0) for token_id in input_ids]\n",
    "\n",
    "        # ä½¿ç”¨ PyThaiNLP ä¾†æŒ‡å°æ³°æ–‡å­è©åˆä½µ\n",
    "        merged_tokens, merged_weights = merge_subwords_th(tokens_str, lexical_weights, thai_words_batch[idx])\n",
    "\n",
    "        key = (location_ids[idx], languages[idx])\n",
    "        for token, weight in zip(merged_tokens, merged_weights):\n",
    "            token = token.lower().strip()\n",
    "            if len(token) < 2 or token in stopwords:\n",
    "                continue\n",
    "            location_topics_weight[key][token] += weight\n",
    "\n",
    "        if len(location_topics_weight[key]) == 0:\n",
    "            location_topics_weight[key][\"(ç„¡é—œéµè©)\"] = 0.0  # é¿å… key æ˜¯ç©ºçš„\n",
    "\n",
    "    # æ¯è™•ç†å®Œä¸€å€‹chunkï¼Œæ¸…ç†è¨˜æ†¶é«”\n",
    "    del df_chunk, comments, location_ids, languages, sparse_weights_list, input_ids_list, thai_words_batch\n",
    "    gc.collect()\n",
    "    chunk_index += 1\n",
    "    print(f\"å®Œæˆç¬¬{chunk_index}å€‹chunkè™•ç†èˆ‡è¨˜æ†¶é«”æ¸…ç†ã€‚\")\n",
    "\n",
    "# å…¨éƒ¨chunkå®Œæˆå¾Œå†ç”¢å‡ºtop30 Excel\n",
    "rows = []\n",
    "for (loc_id, lang), token_weights in location_topics_weight.items():\n",
    "    top30 = sorted(token_weights.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "    top_words_str = \", \".join([f\"{token}:{weight:.2f}\" for token, weight in top30])\n",
    "    rows.append({\n",
    "        \"location_id\": loc_id,\n",
    "        \"gmap_location\": location_mapping.get(loc_id, loc_id),\n",
    "        \"language\": lang,\n",
    "        \"top_30_tokens\": top_words_str\n",
    "    })\n",
    "\n",
    "df_output = pd.DataFrame(rows)\n",
    "df_output.to_excel(\"location_topics_top30_th_.xlsx\", index=False)\n",
    "print(\"è™•ç†å®Œæˆï¼Œå·²å„²å­˜ location_topics_top30_th.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sudachipy sudachidict_full sudachidict_core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已成功過濾並輸出為 cleaned_comments_th.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = 'cleaned_comments_new.csv'  \n",
    "output_file = 'cleaned_comments_th.csv'  \n",
    "\n",
    "# 載入 CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 過濾 \"language\"\n",
    "df_filtered = df[df['language'] == 'th']\n",
    "\n",
    "# 輸出\n",
    "df_filtered.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"已成功過濾並輸出為 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import stopwordsiso\n",
    "import gc\n",
    "\n",
    "\n",
    "from sudachipy import Dictionary\n",
    "tokenizer_ja = Dictionary().create() \n",
    "\n",
    "# 模型載入\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True, use_cuda=True, device=0)\n",
    "\n",
    "# 中文停用詞\n",
    "stopwords = stopwordsiso.stopwords(\"ja\")\n",
    "\n",
    "# SudachiPy 子詞合併函式（批次處理版本）\n",
    "def merge_subwords_ja(tokens, weights, sudachi_words):\n",
    "    \"\"\"\n",
    "    針對日文評論：\n",
    "    - XLM-R tokenizer 負責 subword 切割\n",
    "    - SudachiPy 提供詞彙邊界，幫助合併子詞\n",
    "    \"\"\"\n",
    "    merged_tokens, merged_weights = [], []\n",
    "    idx = 0\n",
    "\n",
    "    for word in sudachi_words:  # 🔹 這裡直接用已經分詞好的 `sudachi_words`\n",
    "        current_weight = 0.0\n",
    "        current_length = 0\n",
    "\n",
    "        while current_length < len(word) and idx < len(tokens):\n",
    "            token = tokens[idx].replace(\"▁\", \"\").replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n",
    "            current_weight += weights[idx]\n",
    "            current_length += len(token)\n",
    "            idx += 1\n",
    "\n",
    "        merged_tokens.append(word)\n",
    "        merged_weights.append(current_weight)\n",
    "\n",
    "    return merged_tokens, merged_weights\n",
    "\n",
    "# 讀取地點對應資訊\n",
    "with open(\"../RAW_DATA/E_838_location_info.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    location_info = json.load(f)\n",
    "location_mapping = {loc[\"location_id\"]: loc[\"gmap_location\"] for loc in location_info}\n",
    "\n",
    "location_topics_weight = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "# 分批處理評論資料 (batch 處理 BGE 和 CKIP)\n",
    "chunksize = 10000\n",
    "reader = pd.read_csv(\"cleaned_comments_run.csv\", chunksize=chunksize)\n",
    "\n",
    "chunk_index = 0\n",
    "\n",
    "for df_chunk in reader:\n",
    "    df_chunk = df_chunk.dropna(subset=[\"comments\"])\n",
    "    print(f\"正在處理第 {chunk_index+1} 個chunk，共 {len(df_chunk)} 筆評論\")\n",
    "\n",
    "    comments = df_chunk[\"comments\"].tolist()\n",
    "    location_ids = df_chunk[\"location_id\"].tolist()\n",
    "    languages = df_chunk[\"language\"].tolist()\n",
    "\n",
    "    # ** 先用 tokenizer 處理 input_ids (避免 KeyError)**\n",
    "    tokenized_inputs = tokenizer(comments, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # ** BGE 模型 batch 處理**\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encode(comments, return_sparse=True, batch_size=128)\n",
    "\n",
    "    sparse_weights_list = outputs[\"lexical_weights\"]  \n",
    "    input_ids_list = tokenized_inputs[\"input_ids\"].tolist()  \n",
    "\n",
    "    # ** SudachiPy 分詞 batch 處理**\n",
    "    sudachi_words_batch = [ [m.surface() for m in tokenizer_ja.tokenize(text)] for text in comments ]\n",
    "\n",
    "    # ** 處理每筆評論**\n",
    "    for idx in range(len(comments)):\n",
    "        sparse_weights = sparse_weights_list[idx]\n",
    "        input_ids = input_ids_list[idx]\n",
    "        tokens_str = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        lexical_weights = [sparse_weights.get(str(token_id), 0.0) for token_id in input_ids]\n",
    "\n",
    "        # 使用 SudachiPy 來指導日文子詞合併（這裡不再 tokenize，因為 sudachi_words_batch[idx] 已經是 list）\n",
    "        merged_tokens, merged_weights = merge_subwords_ja(tokens_str, lexical_weights, sudachi_words_batch[idx])\n",
    "\n",
    "        key = (location_ids[idx], languages[idx])\n",
    "        for token, weight in zip(merged_tokens, merged_weights):\n",
    "            token = token.lower().strip()\n",
    "            if len(token) < 2 or token in stopwords:\n",
    "                continue\n",
    "            location_topics_weight[key][token] += weight\n",
    "\n",
    "    # 每處理完一個chunk，清理記憶體\n",
    "    del df_chunk, comments, location_ids, languages, sparse_weights_list, input_ids_list, sudachi_words_batch\n",
    "    gc.collect()\n",
    "    chunk_index += 1\n",
    "    print(f\"完成第{chunk_index}個chunk處理與記憶體清理。\")\n",
    "\n",
    "# 全部chunk完成後再產出top30 Excel\n",
    "rows = []\n",
    "for (loc_id, lang), token_weights in location_topics_weight.items():\n",
    "    top30 = sorted(token_weights.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "    top_words_str = \", \".join([f\"{token}:{weight:.2f}\" for token, weight in top30])\n",
    "    rows.append({\n",
    "        \"location_id\": loc_id,\n",
    "        \"gmap_location\": location_mapping.get(loc_id, loc_id),\n",
    "        \"language\": lang,\n",
    "        \"top_30_tokens\": top_words_str\n",
    "    })\n",
    "\n",
    "df_output = pd.DataFrame(rows)\n",
    "df_output.to_excel(\"location_topics_top30_ja.xlsx\", index=False)\n",
    "print(\"處理完成，已儲存 location_topics_top30_ja.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 泰文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import stopwordsiso\n",
    "import gc\n",
    "\n",
    "# 模型載入\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True, use_cuda=True, device=0)\n",
    "\n",
    "\n",
    "# 中文停用詞\n",
    "stopwords = stopwordsiso.stopwords(\"th\")\n",
    "\n",
    "# pythainlp 子詞合併函式（批次處理版本）\n",
    "def merge_subwords_th(tokens, weights, text):\n",
    "    \"\"\"\n",
    "    針對泰文評論：\n",
    "    - XLM-R tokenizer 先切 subword\n",
    "    - PyThaiNLP 幫助合併子詞，使詞彙更自然\n",
    "    \"\"\"\n",
    "    if isinstance(text, list):  # 確保 text 是 string 而非 list\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    words = word_tokenize(text, engine=\"newmm\")  \n",
    "    words = [w.strip() for w in words if w.strip()]  # **過濾掉空白詞**\n",
    "\n",
    "    merged_tokens, merged_weights = [], []\n",
    "    idx = 0\n",
    "\n",
    "    for word in words:\n",
    "        current_weight = 0.0\n",
    "        current_length = 0\n",
    "        success = False  # **加入這個變數來確認是否成功匹配**\n",
    "\n",
    "        while current_length < len(word) and idx < len(tokens):\n",
    "            token = tokens[idx].replace(\"▁\", \"\").replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\").strip()\n",
    "            \n",
    "            if not token:  \n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "            current_weight += weights[idx]\n",
    "            current_length += len(token)\n",
    "            idx += 1\n",
    "            success = True  # **如果有任何 token 被加總，代表匹配成功**\n",
    "\n",
    "        # **如果詞彙成功匹配，才加入**\n",
    "        if success and word:\n",
    "            merged_tokens.append(word)\n",
    "            merged_weights.append(current_weight)\n",
    "        elif word:  # **只在完全沒有匹配成功時，才印出警告**\n",
    "            print(f\"merge_subwords_th() 警告：未成功合併詞彙 - {word}\")\n",
    "\n",
    "    return merged_tokens, merged_weights\n",
    "\n",
    "# 讀取地點對應資訊\n",
    "with open(\"../RAW_DATA/E_838_location_info.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    location_info = json.load(f)\n",
    "location_mapping = {loc[\"location_id\"]: loc[\"gmap_location\"] for loc in location_info}\n",
    "\n",
    "location_topics_weight = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "# 分批處理評論資料 (batch 處理 BGE 和 CKIP)\n",
    "chunksize = 10000\n",
    "reader = pd.read_csv(\"cleaned_comments_run.csv\", chunksize=chunksize)\n",
    "\n",
    "chunk_index = 0\n",
    "\n",
    "for df_chunk in reader:\n",
    "    df_chunk = df_chunk.dropna(subset=[\"comments\"])\n",
    "    print(f\"正在處理第 {chunk_index+1} 個chunk，共 {len(df_chunk)} 筆評論\")\n",
    "\n",
    "    comments = df_chunk[\"comments\"].tolist()\n",
    "    location_ids = df_chunk[\"location_id\"].tolist()\n",
    "    languages = df_chunk[\"language\"].tolist()\n",
    "\n",
    "    # ** 先用 tokenizer 處理 input_ids (避免 KeyError)**\n",
    "    tokenized_inputs = tokenizer(comments, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # ** BGE 模型 batch 處理**\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encode(comments, return_sparse=True, batch_size=128)\n",
    "\n",
    "    sparse_weights_list = outputs[\"lexical_weights\"]  \n",
    "    input_ids_list = tokenized_inputs[\"input_ids\"].tolist()  \n",
    "\n",
    "    # ** 依語言選擇合適的分詞方法**\n",
    "    thai_words_batch = []\n",
    "\n",
    "    for text in comments:\n",
    "        thai_words_batch.append(word_tokenize(text, engine=\"newmm\"))  # 泰文用 PyThaiNLP 分詞\n",
    "\n",
    "    # ** 處理每筆評論**\n",
    "    for idx in range(len(comments)):\n",
    "        sparse_weights = sparse_weights_list[idx]\n",
    "        input_ids = input_ids_list[idx]\n",
    "        tokens_str = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        lexical_weights = [sparse_weights.get(str(token_id), 0.0) for token_id in input_ids]\n",
    "\n",
    "        # 使用 PyThaiNLP 來指導泰文子詞合併\n",
    "        merged_tokens, merged_weights = merge_subwords_th(tokens_str, lexical_weights, thai_words_batch[idx])\n",
    "\n",
    "        key = (location_ids[idx], languages[idx])\n",
    "        for token, weight in zip(merged_tokens, merged_weights):\n",
    "            token = token.lower().strip()\n",
    "            if len(token) < 2 or token in stopwords:\n",
    "                continue\n",
    "            location_topics_weight[key][token] += weight\n",
    "\n",
    "        if len(location_topics_weight[key]) == 0:\n",
    "            location_topics_weight[key][\"(無關鍵詞)\"] = 0.0  # 避免 key 是空的\n",
    "\n",
    "    # 每處理完一個chunk，清理記憶體\n",
    "    del df_chunk, comments, location_ids, languages, sparse_weights_list, input_ids_list, thai_words_batch\n",
    "    gc.collect()\n",
    "    chunk_index += 1\n",
    "    print(f\"完成第{chunk_index}個chunk處理與記憶體清理。\")\n",
    "\n",
    "# 全部chunk完成後再產出top30 Excel\n",
    "rows = []\n",
    "for (loc_id, lang), token_weights in location_topics_weight.items():\n",
    "    top30 = sorted(token_weights.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "    top_words_str = \", \".join([f\"{token}:{weight:.2f}\" for token, weight in top30])\n",
    "    rows.append({\n",
    "        \"location_id\": loc_id,\n",
    "        \"gmap_location\": location_mapping.get(loc_id, loc_id),\n",
    "        \"language\": lang,\n",
    "        \"top_30_tokens\": top_words_str\n",
    "    })\n",
    "\n",
    "df_output = pd.DataFrame(rows)\n",
    "df_output.to_excel(\"location_topics_top30_th_.xlsx\", index=False)\n",
    "print(\"處理完成，已儲存 location_topics_top30_th.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
